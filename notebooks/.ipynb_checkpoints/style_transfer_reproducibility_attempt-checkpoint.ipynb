{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "dEUGlrJSB15C",
    "outputId": "f1063d0e-bff4-44f6-e2e0-7ced8d74d8b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtest\n",
      "  Downloading https://files.pythonhosted.org/packages/42/b1/eb572ef3dce32bfcc3bf60be6360cef0d01a58bde0c76f2e205234d1ff08/torchtest-0.5-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtest) (1.5.0+cu101)\n",
      "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtest) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchtest) (1.18.3)\n",
      "Installing collected packages: torchtest\n",
      "Successfully installed torchtest-0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torchtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k3HzPR2AnpN_"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtest'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8e9e9a4e9a1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massert_vars_change\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massert_vars_same\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_suite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgaussian_filter1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtest'"
     ]
    }
   ],
   "source": [
    "\"\"\"IMPORTS\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtest import assert_vars_change, assert_vars_same, test_suite\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJhdMaArAfhc"
   },
   "outputs": [],
   "source": [
    "\"\"\"TESTING CODE BLOCK\"\"\"\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "def get_gradient_norm(model):\n",
    "  norms = 0\n",
    "  for param in model.parameters():\n",
    "    norms += param.grad.data.detach().norm(2)\n",
    "  return norms\n",
    "\n",
    "def visualize_history_norm(history, history_name, zoom_axis=[], sigma=5):\n",
    "  plt.plot(gaussian_filter1d(history, sigma))\n",
    "  plt.xlabel('Iteration')\n",
    "  plt.ylabel(history_name)\n",
    "  if zoom_axis:\n",
    "    plt.axis(zoom_axis)\n",
    "  plt.show()\n",
    "\n",
    "# this function should take a batch from the dataloader and:\n",
    "#\n",
    "# pass the batch through the model all at once and collect\n",
    "# that output in a variable\n",
    "#\n",
    "# pass each item in the batch through the model by itself \n",
    "# and collect the output such that it's the same shape as the\n",
    "# original batch\n",
    "#\n",
    "# compare these two using torch.allclose to make sure they are\n",
    "# the same!\n",
    "def test_forward_pass(model, dataloader):\n",
    "  data, _ = next(iter(dataloader))\n",
    "  output_batch = model(data.float())\n",
    "  output_single = []\n",
    "  for datum in data:\n",
    "    output_single.append(model(datum.float()))\n",
    "\n",
    "  assert torch.allclose(output_batch, torch.cat(output_single)), 'Forward pass is batch dependent'\n",
    "\n",
    "# this function should take a batch from the dataloader and:\n",
    "#\n",
    "# pass the batch through the model all at once then do a \n",
    "# backwards and collect the gradient\n",
    "#\n",
    "# pass each item in the batch through the model by itself \n",
    "# do backwards on each item (accummulating the gradient),\n",
    "# and collect the gradient at the end\n",
    "#\n",
    "# compare these two using torch.allclose to make sure they are\n",
    "# the same!\n",
    "def test_backward_pass(model, dataloader, loss):\n",
    "  model.zero_grad()\n",
    "  data, targets = next(iter(dataloader))\n",
    "  _loss_batch = loss(model(data.float()), targets.long())\n",
    "  _loss_batch.backward()\n",
    "  accumulated_batch = get_gradient_norm(model)\n",
    "  \n",
    "  model.zero_grad() \n",
    "  for datum, target in zip(data, targets):\n",
    "    _loss_single = loss(model(datum.float()), target.long().reshape(1))\n",
    "    _loss_single.backward()\n",
    "  accumulated_single = get_gradient_norm(model)/data.shape[0]\n",
    "  \n",
    "  assert torch.allclose(accumulated_batch.reshape(1), accumulated_single.reshape(1), atol=1e-3), 'loss function is cross-linking data'\n",
    "  \n",
    "def test_gradient_flow(model, dataloader, loss, magnitude=-5, compare_prev_layers=True, compare_prev_layers_magnitude=3):\n",
    "  # pass data through the model, then compare the gradient at each\n",
    "  # layer in the model. the gradient should never become really\n",
    "  # tiny, as this means the earlier layers of the model will be\n",
    "  # tough to train. your network is probably too deep!\n",
    "  \n",
    "  model.zero_grad()\n",
    "  data, targets = next(iter(dataloader))\n",
    "  _loss_batch = loss(model(data.float()), targets.long())\n",
    "  _loss_batch.backward()\n",
    "  grad_norms = []\n",
    "  for param in model.parameters():\n",
    "    grad_norms.append(param.grad.data.norm(2).detach())\n",
    "  last_norm = grad_norms[-1]\n",
    "  for norm in reversed(grad_norms[:(len(grad_norms) - 1)]):\n",
    "    if comparelayers:\n",
    "      assert not torch.log10(last_norm) - torch.log10(norm) > compare_prev_layers_magnitude, 'Early gradients vanish too quickly compared to later layers'\n",
    "    assert torch.log10(last_norm) > magnitude, 'Tensor magnitude is too small'\n",
    "    last_norm = norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jzC-D7hHa8kt"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image # python image library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import copy\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "imsize = 512 if torch.cuda.is_available() else 128\n",
    "loader = transforms.Compose([transforms.Resize(imsize), transforms.ToTensor()])\n",
    "\n",
    "def image_loader(image_name):\n",
    "  image = Image.open(image_name)\n",
    "  image = loader(image)\n",
    "  print('Images shape initial', image.shape)\n",
    "  image = image.unsqueeze(0) # gets right dimension at particular size\n",
    "  print('Image shape after unsqueeze', image.shape)\n",
    "  return image.to(device, torch.float)\n",
    "\n",
    "%cd /content/drive/My Drive/temp_images\n",
    "style_img = image_loader('picasso.jpg')\n",
    "content_img = image_loader('dancing.jpg')\n",
    "\n",
    "assert style_img.size() == content_img.size(), 'Incorrect Sizing'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4cBrD8z69MXs"
   },
   "outputs": [],
   "source": [
    "unloader = transforms.ToPILImage() # converts an image from a tensor to a PIL image\n",
    "plt.ion()\n",
    "\n",
    "# simple method to graph PIL images\n",
    "def imshow(tensor, title=None):\n",
    "  image = tensor.cpu().clone # clone the tensor, prevent pass by ref\n",
    "  image = image.squeeze(0) # index 0 is the batch dimension\n",
    "  image = unloader(image)\n",
    "  plt.imshow(image)\n",
    "  if not title:\n",
    "    plt.title(title)\n",
    "  plt.pause(0.001) # pausing to update the plots \n",
    "\n",
    "# graphing the image we want to style, and the style ref\n",
    "plt.figure()\n",
    "imshow(style_img, title='Style Image')\n",
    "\n",
    "plt.figure()\n",
    "imshow(content_img, title='Content Image')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZaTS71Z6_LvD"
   },
   "outputs": [],
   "source": [
    "# style transfer uses two different loss functions which will be important\n",
    "# for getting good results. really important for deep-style-transfer(DST)\n",
    "# \n",
    "# Content Loss: Returns the weighted content distance from the feature maps of \n",
    "# a layer (l) processing input (x). distance from x to content image (c)\n",
    "# MSE from Feature maps of x to c\n",
    "\n",
    "\n",
    "# !!! this is not a PyTorch Loss function, we need to create an autograd \n",
    "#     function and implement the gradient in the backward method !!!\n",
    "class ContentLoss(nn.Module):\n",
    "\n",
    "  def __init__(self, target,):\n",
    "    super(ContentLoss, self).__init__()\n",
    "    self.target = target.detach()\n",
    "\n",
    "  def forward(self, input):\n",
    "    self.loss = F.mse_loss(input, self.target)\n",
    "    return input\n",
    "\n",
    "# helper function for the style loss, very helpful for DST\n",
    "# gram matrix https://en.wikipedia.org/wiki/Gramian_matrix \n",
    "# normalization is really important to prevent early layers from impacting \n",
    "# the output, style is learned in deeper layers\n",
    "\n",
    "def gram_matrix(input):\n",
    "  a, b, c ,d = input.size()\n",
    "  # a batch size\n",
    "  # b number of feature maps\n",
    "  # c, d | dimensions of a feature map (N=c*d)\n",
    "\n",
    "  features = input.view(a*b*c*d) # resizing feature map of the layer\n",
    "  G = torch.mm(features, features.t()) # gram product calculation\n",
    "  # normalization here\n",
    "  G_normalized = G.div(a*b*c*d)\n",
    "  # we can play with results non-normalized to see the impact on the output\n",
    "  return G_normalized  \n",
    "\n",
    "# Style Loss: like content loss, uses MSE between two gram matricies\n",
    "class StyleLoss(nn.Module): \n",
    "  def __init__(self, target_features):\n",
    "    super(StyleLoss, self).__init__()\n",
    "    # .detach means that this tensor does not need grad \n",
    "    self.target = gram_matrix(target_feature).detach()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uv7zUAsyA8Dy"
   },
   "outputs": [],
   "source": [
    "# READ THIS DECISION FOR WEEK 5\n",
    "# next we need to build a 19 layer VGG network here, or use the PyTorch one\n",
    "# I think rebuilding the network will teach us a lot and give us more room to \n",
    "# customize VGG\n",
    "# here is pre-made PyTorch network imported\n",
    "cnn = model.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "# VGG networks are trained on images where each color channel is normalized by \n",
    "# mean=[.485, .456, .406] and a std=[.229, .224,.225] \n",
    "\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tenseor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "# model to normalize input so we can use it to quickly normalize a sequential \n",
    "# model\n",
    "class Normalization(nn.Module):\n",
    "  def __init__(self, mean, std):\n",
    "    super(Normalization, self).__inti__()\n",
    "    self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "    self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "  def forward(self, img):\n",
    "    # normalize the image \n",
    "    return (img - self.mean)/ self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_content_layers = ['conv_4']\n",
    "default_style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4','conv_5' ]\n",
    "\n",
    "def get_style_model_and_loss(cnn, normalization_mean, normalization_std\n",
    "                            style_img, content_img,\n",
    "                             content_layers=default_content_layers,\n",
    "                             style_layers=default_style_layers):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    \n",
    "    # using the normalization class to normalize mean and std\n",
    "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "    \n",
    "    # list made for accumulating losses\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "    \n",
    "    # using nn.Sequential because we assume that the cnn is already a \n",
    "    # nn.Sequential, this lets us put modules to be activated sequentially\n",
    "    model = nn.Sequential(normalization)\n",
    "    \n",
    "    number_of_conv = 0 \n",
    "    \n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            number_of_conv += 1\n",
    "            name = f'conv{number_of_conv}'\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = f'relu_{number_of_conv}'\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = f'pool_{number_of_conv}'\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = f'bn_{number_of_conv}'\n",
    "        else:\n",
    "            raise RuntimeError('Unrecongnized layer:'.format(layer.__class__.__name__))\n",
    "        model.add_module(name, layer)\n",
    "        \n",
    "        if name in content_ayers:\n",
    "            # adding to the conetent loss\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(f'content_loss_{number_of_conv}', content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "            \n",
    "        if name in style_layers:\n",
    "            # adding to the style loss\n",
    "            target_features = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(f'style_loss{number_of_conv}', style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "        # trim off layers after the last content and style losses\n",
    "        for i in range(len(model) -1, -1, -1):\n",
    "            if isinstance(model[number_of_conv], ContentLoss) or isinstance(model[number_of_conv], StyleLoss):\n",
    "                break\n",
    "        model = model[:(number_of_conv+1)]\n",
    "        \n",
    "        return model, style_losses, content_losses\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_optimizer(input_img):\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
    "                      content_img, style_img, num_steps=300,\n",
    "                      style_weight=1000000, content_weight=1):\n",
    "    print('Building the model')\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn, normalization_mean\n",
    "                                                                    normalization_std, style_img,\n",
    "                                                                     input_img, num_steps=300,\n",
    "                                                                     style_weight=1000000, content_weight=1)\n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "    \n",
    "    print('Optimizing...')\n",
    "    run = [0]\n",
    "    while run[0] <= num_stpes:\n",
    "        \n",
    "        def closure():\n",
    "            input_img_.data.clamp_(0, 1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "            \n",
    "            for sl in style_losses:\n",
    "                style_score += sl.loss\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "                \n",
    "            style_score *= style_weight\n",
    "            content_score *= content_weight\n",
    "            \n",
    "            loss = style_score + content_score\n",
    "            loss.backward()\n",
    "            \n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(f'run {run}')\n",
    "                print(f'Style Loss: {style_score.item()} Content Loss {content_score.item()}')\n",
    "                print()\n",
    "                \n",
    "            return style_score + content_score\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "input_img.data.clamp_(0, 1)\n",
    "\n",
    "return input_img"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "style_transfer_reproducibility_attempt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "neural-networks",
   "language": "python",
   "name": "neural-networks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
