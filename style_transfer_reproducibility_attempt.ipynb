{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "style_transfer_reproducibility_attempt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEUGlrJSB15C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6349d25e-5316-4879-ae9e-30ae75909ead"
      },
      "source": [
        "!pip install --upgrade torchtest"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtest\n",
            "  Downloading https://files.pythonhosted.org/packages/42/b1/eb572ef3dce32bfcc3bf60be6360cef0d01a58bde0c76f2e205234d1ff08/torchtest-0.5-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtest) (1.4.0)\n",
            "Installing collected packages: torchtest\n",
            "Successfully installed torchtest-0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3HzPR2AnpN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"IMPORTS\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torchtest import assert_vars_change, assert_vars_same, test_suite\n",
        "from scipy.ndimage import gaussian_filter1d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJhdMaArAfhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "def get_gradient_norm(model):\n",
        "  norms = 0\n",
        "  for param in model.parameters():\n",
        "    norms += param.grad.data.detach().norm(2)\n",
        "  return norms\n",
        "\n",
        "def visualize_history_norm(history, history_name, zoom_axis=[], sigma=5):\n",
        "  plt.plot(gaussian_filter1d(history, sigma))\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel(history_name)\n",
        "  if zoom_axis:\n",
        "    plt.axis(zoom_axis)\n",
        "  plt.show()\n",
        "\n",
        "# this function should take a batch from the dataloader and:\n",
        "#\n",
        "# pass the batch through the model all at once and collect\n",
        "# that output in a variable\n",
        "#\n",
        "# pass each item in the batch through the model by itself \n",
        "# and collect the output such that it's the same shape as the\n",
        "# original batch\n",
        "#\n",
        "# compare these two using torch.allclose to make sure they are\n",
        "# the same!\n",
        "def test_forward_pass(model, dataloader):\n",
        "  data, _ = next(iter(dataloader))\n",
        "  output_batch = model(data.float())\n",
        "  output_single = []\n",
        "  for datum in data:\n",
        "    output_single.append(model(datum.float()))\n",
        "\n",
        "  assert torch.allclose(output_batch, torch.cat(output_single))\n",
        "\n",
        "# this function should take a batch from the dataloader and:\n",
        "#\n",
        "# pass the batch through the model all at once then do a \n",
        "# backwards and collect the gradient\n",
        "#\n",
        "# pass each item in the batch through the model by itself \n",
        "# do backwards on each item (accummulating the gradient),\n",
        "# and collect the gradient at the end\n",
        "#\n",
        "# compare these two using torch.allclose to make sure they are\n",
        "# the same!\n",
        "def test_backward_pass(model, dataloader, loss):\n",
        "  model.zero_grad()\n",
        "  data, targets = next(iter(dataloader))\n",
        "  _loss_batch = loss(model(data.float()), targets.long())\n",
        "  _loss_batch.backward()\n",
        "  accumulated_batch = get_gradient_norm(model)\n",
        "  \n",
        "  model.zero_grad() \n",
        "  for datum, target in zip(data, targets):\n",
        "    _loss_single = loss(model(datum.float()), target.long().reshape(1))\n",
        "    _loss_single.backward()\n",
        "  accumulated_single = get_gradient_norm(model)/data.shape[0]\n",
        "  \n",
        "  assert torch.allclose(accumulated_batch.reshape(1), accumulated_single.reshape(1), atol=1e-3)\n",
        "  \n",
        "def test_gradient_flow(model, dataloader, loss, magnitude=-5, compare_prev_layers=True, compare_prev_layers_magnitude=3):\n",
        "  # pass data through the model, then compare the gradient at each\n",
        "  # layer in the model. the gradient should never become really\n",
        "  # tiny, as this means the earlier layers of the model will be\n",
        "  # tough to train. your network is probably too deep!\n",
        "  \n",
        "  model.zero_grad()\n",
        "  data, targets = next(iter(dataloader))\n",
        "  _loss_batch = loss(model(data.float()), targets.long())\n",
        "  _loss_batch.backward()\n",
        "  grad_norms = []\n",
        "  for param in model.parameters():\n",
        "    grad_norms.append(param.grad.data.norm(2).detach())\n",
        "  last_norm = grad_norms[-1]\n",
        "  for norm in reversed(grad_norms[:(len(grad_norms) - 1)]):\n",
        "    if comparelayers:\n",
        "      assert not torch.log10(last_norm) - torch.log10(norm) > compare_prev_layers_magnitude\n",
        "    assert torch.log10(last_norm) > magnitude\n",
        "    last_norm = norm"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}