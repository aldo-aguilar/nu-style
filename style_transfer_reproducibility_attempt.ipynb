{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "style_transfer_reproducibility_attempt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "neural-networks",
      "language": "python",
      "name": "neural-networks"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dEUGlrJSB15C",
        "outputId": "f1063d0e-bff4-44f6-e2e0-7ced8d74d8b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "!pip install --upgrade torchtest"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtest\n",
            "  Downloading https://files.pythonhosted.org/packages/42/b1/eb572ef3dce32bfcc3bf60be6360cef0d01a58bde0c76f2e205234d1ff08/torchtest-0.5-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtest) (1.5.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtest) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchtest) (1.18.3)\n",
            "Installing collected packages: torchtest\n",
            "Successfully installed torchtest-0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k3HzPR2AnpN_",
        "colab": {}
      },
      "source": [
        "\"\"\"IMPORTS\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torchtest import assert_vars_change, assert_vars_same, test_suite\n",
        "from scipy.ndimage import gaussian_filter1d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zJhdMaArAfhc",
        "colab": {}
      },
      "source": [
        "\"\"\"TESTING CODE BLOCK\"\"\"\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def get_gradient_norm(model):\n",
        "  norms = 0\n",
        "  for param in model.parameters():\n",
        "    norms += param.grad.data.detach().norm(2)\n",
        "  return norms\n",
        "\n",
        "def visualize_history_norm(history, history_name, zoom_axis=[], sigma=5):\n",
        "  plt.plot(gaussian_filter1d(history, sigma))\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel(history_name)\n",
        "  if zoom_axis:\n",
        "    plt.axis(zoom_axis)\n",
        "  plt.show()\n",
        "\n",
        "# this function should take a batch from the dataloader and:\n",
        "#\n",
        "# pass the batch through the model all at once and collect\n",
        "# that output in a variable\n",
        "#\n",
        "# pass each item in the batch through the model by itself \n",
        "# and collect the output such that it's the same shape as the\n",
        "# original batch\n",
        "#\n",
        "# compare these two using torch.allclose to make sure they are\n",
        "# the same!\n",
        "def test_forward_pass(model, dataloader):\n",
        "  data, _ = next(iter(dataloader))\n",
        "  output_batch = model(data.float())\n",
        "  output_single = []\n",
        "  for datum in data:\n",
        "    output_single.append(model(datum.float()))\n",
        "\n",
        "  assert torch.allclose(output_batch, torch.cat(output_single)), 'Forward pass is batch dependent'\n",
        "\n",
        "# this function should take a batch from the dataloader and:\n",
        "#\n",
        "# pass the batch through the model all at once then do a \n",
        "# backwards and collect the gradient\n",
        "#\n",
        "# pass each item in the batch through the model by itself \n",
        "# do backwards on each item (accummulating the gradient),\n",
        "# and collect the gradient at the end\n",
        "#\n",
        "# compare these two using torch.allclose to make sure they are\n",
        "# the same!\n",
        "def test_backward_pass(model, dataloader, loss):\n",
        "  model.zero_grad()\n",
        "  data, targets = next(iter(dataloader))\n",
        "  _loss_batch = loss(model(data.float()), targets.long())\n",
        "  _loss_batch.backward()\n",
        "  accumulated_batch = get_gradient_norm(model)\n",
        "  \n",
        "  model.zero_grad() \n",
        "  for datum, target in zip(data, targets):\n",
        "    _loss_single = loss(model(datum.float()), target.long().reshape(1))\n",
        "    _loss_single.backward()\n",
        "  accumulated_single = get_gradient_norm(model)/data.shape[0]\n",
        "  \n",
        "  assert torch.allclose(accumulated_batch.reshape(1), accumulated_single.reshape(1), atol=1e-3), 'loss function is cross-linking data'\n",
        "  \n",
        "def test_gradient_flow(model, dataloader, loss, magnitude=-5, compare_prev_layers=True, compare_prev_layers_magnitude=3):\n",
        "  # pass data through the model, then compare the gradient at each\n",
        "  # layer in the model. the gradient should never become really\n",
        "  # tiny, as this means the earlier layers of the model will be\n",
        "  # tough to train. your network is probably too deep!\n",
        "  \n",
        "  model.zero_grad()\n",
        "  data, targets = next(iter(dataloader))\n",
        "  _loss_batch = loss(model(data.float()), targets.long())\n",
        "  _loss_batch.backward()\n",
        "  grad_norms = []\n",
        "  for param in model.parameters():\n",
        "    grad_norms.append(param.grad.data.norm(2).detach())\n",
        "  last_norm = grad_norms[-1]\n",
        "  for norm in reversed(grad_norms[:(len(grad_norms) - 1)]):\n",
        "    if comparelayers:\n",
        "      assert not torch.log10(last_norm) - torch.log10(norm) > compare_prev_layers_magnitude, 'Early gradients vanish too quickly compared to later layers'\n",
        "    assert torch.log10(last_norm) > magnitude, 'Tensor magnitude is too small'\n",
        "    last_norm = norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jzC-D7hHa8kt",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "READ THIS!!\n",
        "\n",
        "Since the deep photo style transfer (DPST) builds on the neural transfer algorithm\n",
        "and we are focusing on DPST, I figure it would be unnecessary to try to \n",
        "reproduce the neural style from the original paper. Instead, I am following this tutorial:\n",
        "\n",
        "https://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image # python image library\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import copy\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "imsize = 512 if torch.cuda.is_available() else 128\n",
        "loader = transforms.Compose([transforms.Resize(imsize), transforms.ToTensor()])\n",
        "\n",
        "def image_loader(image_name):\n",
        "  image = Image.open(image_name)\n",
        "  image = loader(image)\n",
        "  print('Images shape initial', image.shape)\n",
        "  image = image.unsqueeze(0) # gets right dimension at particular size\n",
        "  print('Image shape after unsqueeze', image.shape)\n",
        "  return image.to(device, torch.float)\n",
        "\n",
        "%cd /content/drive/My Drive/temp_images\n",
        "style_img = image_loader('picasso.jpg')\n",
        "content_img = image_loader('dancing.jpg')\n",
        "\n",
        "assert style_img.size() == content_img.size(), 'Incorrect Sizing'\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cBrD8z69MXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unloader = transforms.ToPILImage() # converts an image from a tensor to a PIL image\n",
        "plt.ion()\n",
        "\n",
        "# simple method to graph PIL images\n",
        "def imshow(tensor, title=None):\n",
        "  image = tensor.cpu().clone # clone the tensor, prevent pass by ref\n",
        "  image = image.squeeze(0) # index 0 is the batch dimension\n",
        "  image = unloader(image)\n",
        "  plt.imshow(image)\n",
        "  if not title:\n",
        "    plt.title(title)\n",
        "  plt.pause(0.001) # pausing to update the plots \n",
        "\n",
        "# graphing the image we want to style, and the style ref\n",
        "plt.figure()\n",
        "imshow(style_img, title='Style Image')\n",
        "\n",
        "plt.figure()\n",
        "imshow(content_img, title='Content Image')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaTS71Z6_LvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# style transfer uses two different loss functions which will be important\n",
        "# for getting good results. really important for deep-style-transfer(DST)\n",
        "# \n",
        "# Content Loss: Returns the weighted content distance from the feature maps of \n",
        "# a layer (l) processing input (x). distance from x to content image (c)\n",
        "# MSE from Feature maps of x to c\n",
        "\n",
        "\n",
        "# !!! this is not a PyTorch Loss function, we need to create an autograd \n",
        "#     function and implement the gradient in the backward method !!!\n",
        "class ContentLoss(nn.Module):\n",
        "\n",
        "  def __init__(self, target,):\n",
        "    super(ContentLoss, self).__init__()\n",
        "    self.target = target.detach()\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.loss = F.mse_loss(input, self.target)\n",
        "    return input\n",
        "\n",
        "# helper function for the style loss, very helpful for DST\n",
        "# gram matrix https://en.wikipedia.org/wiki/Gramian_matrix \n",
        "# normalization is really important to prevent early layers from impacting \n",
        "# the output, style is learned in deeper layers\n",
        "\n",
        "def gram_matrix(input):\n",
        "  a, b, c ,d = input.size()\n",
        "  # a batch size\n",
        "  # b number of feature maps\n",
        "  # c, d | dimensions of a feature map (N=c*d)\n",
        "\n",
        "  features = input.view(a*b*c*d) # resizing feature map of the layer\n",
        "  G = torch.mm(features, features.t()) # gram product calculation\n",
        "  # normalization here\n",
        "  G_normalized = G.div(a*b*c*d)\n",
        "  # we can play with results non-normalized to see the impact on the output\n",
        "  return G_normalized  \n",
        "\n",
        "# Style Loss: like content loss, uses MSE between two gram matricies\n",
        "class StyleLoss(nn.Module): \n",
        "  def __init__(self, target_features):\n",
        "    super(StyleLoss, self).__init__()\n",
        "    # .detach means that this tensor does not need grad \n",
        "    self.target = gram_matrix(target_feature).detach()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv7zUAsyA8Dy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# READ THIS DECISION FOR WEEK 5\n",
        "# next we need to build a 19 layer VGG network here, or use the PyTorch one\n",
        "# I think rebuilding the network will teach us a lot and give us more room to \n",
        "# customize VGG\n",
        "# here is pre-made PyTorch network imported\n",
        "cnn = model.vgg19(pretrained=True).features.to(device).eval()\n",
        "\n",
        "# VGG networks are trained on images where each color channel is normalized by \n",
        "# mean=[.485, .456, .406] and a std=[.229, .224,.225] \n",
        "\n",
        "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "cnn_normalization_std = torch.tenseor([0.229, 0.224, 0.225]).to(device)\n",
        "\n",
        "# model to normalize input so we can use it to quickly normalize a sequential \n",
        "# model\n",
        "class Normalization(nn.Module):\n",
        "  def __init__(self, mean, std):\n",
        "    super(Normalization, self).__inti__()\n",
        "    self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "    self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "  def forward(self, img):\n",
        "    # normalize the image \n",
        "    return (img - self.mean)/ self.std"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}